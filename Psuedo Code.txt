




#Initial Data in an array 



#Class 1 Tier 1 -> Piping all data with a given attribute are placed here 



#Download all 5 Data sets 


++PREPROCESSING - i.e. get real familiar with Pandas

1. provide program with Dataset (command line arg)
2. import dataset as pandas dataframe
3. check dataset for missing attributes ("?", "NaN", etc.)
    # https://thispointer.com/pandas-get-frequency-of-a-value-in-dataframe-column-index-find-its-positions-in-python/
    a. if only small percent of examples have missing attributes, remove those examples.
    b. if only a small fraction of columns (e.g. 2/12) have missing attributes, remove those columns. 
    c. if many datapoints across many columns have missing attributes, generate at random to match column distribution. 
        i. find attribute value distribution across discrete options (find min/max?) Use pandas stats for this
4. check dataset for continuous-valued attribute values
    a. if continuous valued, bin all continous-valued columns with pandas functions
5. copy the dataset. In the new set, shuffle the values at random for 10% of the features

++EXPERIMENT 

##TRAINING

1. Create datastructure to store calculated values
2. Divide dataset into training and test sets (5x2cv?)
3. iterate over dataframe to calculate Naive-Bayes values, store values. 

##TESTING

1. calculate C(x) for each element in the test set. Store class prediction. 




Datasets that need to missing attributes filled in/modified:
    -Cancer
    -Voting

Datasets that need to be discretized:
    -Iris
    -Glass


overall driver:

df = data from file - preprocessed
ta = Training_algorithm.new()
n = ta.calculateN(df)
q = ta.calculateQ(n, length of df)

f = ta.calculateF(df, n)

td = testData (this is a dataframe)
td.addColumn
cl = Classifier.new(f)

results = cl.classify(td) (returns a dataframe)

print(results)